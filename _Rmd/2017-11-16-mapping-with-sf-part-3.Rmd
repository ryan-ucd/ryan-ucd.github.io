---
categories: blog
comments: true
use-site-title: false
date: "2017-11-16"
layout: post
tags: [R, maps, GIS]
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
knitr::opts_knit$set(root.dir = normalizePath("../"))

```

## Part III: Snapping Download NHD Flowline Data and Use `sf` Functions

  Though it's a bit late in getting posted, this is part III in a spatial series that is becoming a quasi-workshop in the variety of ways we can work with spatial river & stream data in R. This post will continue using the `sf` package, and introduce the `riverdist` package, but we'll still be playing with data on rivers and streams. 
  
  To recap, in the first few posts we made a leaflet map, scraped data from tables on a webpage, demonstrated how to calculate distance matrices between point data, and showed how to use functions written by folks at USGS (OWI) as well as the Hydrosheds data to download riverline data. That's a fair amount to chew on I realize, but I'd like to demo a few additional things that might be useful for folks working with river data. 
  
**This post will (*try to*) show you:**
  
 - How to use the `riverdist` package to process a riverline network
 - How to snap spatial point data to the processed riverline data (which we downloaded in the previous post)
 - Finally, how to calculate the river distance between sites (so along the river network)
 - and a quick `ggmap` example
 
I think in a future post I'd like to demo how we can conduct a pretty nifty wavelet analysis of these flow data, and use that to assess patterns in river impairment. Too many things to do. Onward.

### Load the Packages

The main packages I'm going to use in this post:

```{r packages, eval=T, echo=T}

# load libraries
suppressMessages({
  library(dplyr); # data munging and piping
  library(purrr); # for list functions
  library(ggplot2); # plotting
  library(ggrepel) # for labeling
  library(sf); # spatial simple features
  library(USAboundaries); # state/county data
  library(Imap); # nice mapping/color functions
  #library(geoknife); # USGS tool set (Next post)
  #library(dataRetrieval); # USGS tool set (next post)
  library(httr) # scraping webdata (to download flowlines)
})

# Note, to plot `sf` objects with `ggplot`, you need to have the most recent version (>2.2.1). A quick and easy check you can add to your code is an `if` statement: `if (utils::packageVersion("ggplot2") > "2.2.1"))`
```

### Load the NHDFlowline Function

Let's load the `get_flowline` function from our previous post so we can have it accessible should we need it. This function was from an excellent gist by Laura DeCicco (see [here](https://gist.github.com/ldecicco-USGS/56262f3809f0807cb523d7105cb790a9)), and I modified it slightly to work with the `sf` package.

```{r getNHD_flowlines_function, eval=T, echo=T}

library(httr)

get_flowlines <- function(streamorder, mapRange){
  postURL <- "https://cida.usgs.gov/nwc/geoserver/nhdplus/ows"
  
  filterXML <- paste0('<?xml version="1.0"?>',
                '<wfs:GetFeature xmlns:wfs="http://www.opengis.net/wfs" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:gml="http://www.opengis.net/gml" service="WFS" version="1.1.0" outputFormat="shape-zip" xsi:schemaLocation="http://www.opengis.net/wfs http://schemas.opengis.net/wfs/1.1.0/wfs.xsd">',
                  '<wfs:Query xmlns:feature="https://gov.usgs.cida/nhdplus" typeName="feature:nhdflowline_network" srsName="EPSG:4326">',
                    '<ogc:Filter xmlns:ogc="http://www.opengis.net/ogc">',
                      '<ogc:And>',
                        '<ogc:PropertyIsGreaterThan>',
                          '<ogc:PropertyName>streamorde</ogc:PropertyName>',
                          '<ogc:Literal>',streamorder-1,'</ogc:Literal>',
                        '</ogc:PropertyIsGreaterThan>',
                        '<ogc:BBOX>',
                          '<ogc:PropertyName>the_geom</ogc:PropertyName>',
                          '<gml:Envelope>',
                            '<gml:lowerCorner>',mapRange[3]," ",mapRange[1],'</gml:lowerCorner>',
                            '<gml:upperCorner>',mapRange[4]," ",mapRange[2],'</gml:upperCorner>',
                          '</gml:Envelope>',
                        '</ogc:BBOX>',
                      '</ogc:And>',
                    '</ogc:Filter>',
                  '</wfs:Query>',
                '</wfs:GetFeature>')

  destination = file.path(tempdir(),"nhdflowline_network.zip")
  file <- POST(postURL, body = filterXML, write_disk(destination, overwrite=T))

  filePath <- tempdir()
  print("unzipping...")
  unzip(destination, exdir = filePath)
  
  flowLines <- st_read(filePath, layer = 'nhdflowline_network')

  return(flowLines)
}

```

# Calculating River Distances

To use the `riverdist` package, we need to load a shapefile with a river network of some type. This example uses NHD Streamline data, cropped to the North Fork American watershed. You can download the data [here](https://github.com/ryanpeek/ryanpeek.github.io/blob/master/data/nhd_rivs_nfa_ord_1_4.rda). To save time, let's use [HUC8 data](https://github.com/ryanpeek/test_projects/blob/master/data/shps/h8_AMR_BEA_YUB.zip) from the previous post. Download it to a "`data`" folder and you can follow along.

```{r loadHUCflowdata, echo=T, eval=T}

# load the streamline data
load("data/nhd_rivs_ord_1_4.rda")

# load the hucs
huc8 <- read_sf(unzip("data/h8_AMR_BEA_YUB.zip"), quiet = F) %>%
  st_transform(crs=4326) #%>% 

# then remove raw files since file is added in memory
file.remove(list.files(pattern = "h8_AMR_BEA_YUB*",recursive = F))

h8 <- huc8 %>% filter(HU_8_NAME=="North Fork American") 

# plot 
plot(st_geometry(h8), axes=T, lwd = 3)
plot(st_geometry(rivers2), col="blue", add=T)
```

Ok, well let's keep this simple, use only NHD `Stream Order=2` data and crop these flowline data to the NF American Watershed. Then we can project and write the data as a shapefile.

```{r cropAndSave}

rivs2 <- st_intersection(rivers2, h8)

# plot to check:
plot(st_geometry(h8), axes=T, lwd=3)
plot(st_geometry(rivs2), col="blue", add=T)

# okay looks good lets save:
st_write(rivs2, "data/nfa_rivers2.shp")

```

You might see a warning message about attribute field names, that's okay. File should be where you saved it and have 4 separate files (`.dbf`, `.prj`, `.shp`, `.shx`). There can be more associated with a shapefile, but this is the barebones required.

## Clean River Network Topology

Now that we have a river network, we need to clean it and check for errors so that when the algorithm is routing things along the network, the distances will be more accurate.

The main steps are:

 - Load data and transform into a projected coordinate system
 - Plot and Check Topology
 - Clean the network topology so we can snap points 
 - Calculate distances along the river network between sites

```{r rivernetwork, echo=T, eval=T}

# load the package
library(riverdist)

# riverdist only works w/ projected coordinate systems...try:
## EPSG:102003 USA_Contiguous_Albers_Equal_Area_Conic
## +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs
## EPSG:102004 USA_Contiguous_Lambert_Conformal_Conic
## EPSG:102005 USA_Contiguous_Equidistant_Conic

rivs <- line2network(path="data/", layer="nfa_rivers2", reproject = "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs")

# should see message about "Removed 36 Segments w short lengths"

# plot of all the segments
plot(x=rivs)
```

Ok, so we should see a whole bunch of rainbow colored segments there. If we want to look at topology, we can plot the nodes (confluences). Green dots represent connections or confluences between segments, red dots are "headwaters" or isolated branches.

```{r topologyNodes, echo=T, eval=T}

# check topology (nodes)
topologydots(rivers=rivs)

```

### Cleanup: Insert Vertices and Identify River Mouth

The `riverdist` tutorial/vignette is pretty good and will walk folks through the main steps. For larger river networks, the finer scale data you have the more likely you can end up with weird and even persistent things you need to clean. Be aware this cleaning of topology is a stepwise process and it can take some time, but once you have everything cleaned up, you can save the data and re-use network in the future.

I'll try to screen shot this process below:

```{r riverdist_cleanup, eval=T, echo=T}

# do a clean up:
rivs_fixed <- cleanup(rivers = rivs)

```
![step1](../img/histogram_riverdist.png)
```
Dissolving... 
Simplified from 615 to 243 segments. 

 Checking for segments with length less than the connectivity tolerance... 

 0 microsegments identified. 

 Checking if splitting segments is needed... 
Checking between-vertex lengths... 
Maximum distance between vertices is 772 
Insert vertices to reduce distances between vertices and increase point snapping precision? (y/n) 
```

Select "**`y`**", and pick a number...I picked 100 meters. The output will then say it's inserting vertices, followed by:

`Please identify segment number of river mouth:`
![step2](../img/select_river_mouth.png)

Here I've selected **segment 192**, and then selected the mouth as *1*. Once the Network River mouth has been identified, the program will ask ifyou want to remove any additional segments. I select **n** here, but many options.

### Cleanup: Remove Additional Segments/Braiding

Just walk through and determine what you want to keep or not. If there are braided segment, you'll need to pick out the components you want to keep and which to delete. This can be a bit tedious (especially with very fine-scale datasets), but once you walk through, you'll be able to save your updated network.

### Save the Cleaned Data

According to `riverdist`, we should save our cleaned topology file as a `Rdata` file. So:

```{r save_fixed_topology, echo=T, eval=T}
save(rivs_fixed, file = "data/nfa_rivs2_fixed.rda")
```

### Plot Cleaned Topology

Let's take a look at what we did, and why it matters.

```{r compare_fixed_topology, echo=T, eval=T}

load("data/nfa_rivs2_fixed.rda")

# now plot the old vs. the new
par(mfrow=c(1,2))
topologydots(rivers=rivs)
graphics::title("Raw River Topology", family="Roboto Condensed")
topologydots(rivs_fixed)
graphics::title("Clean River Topology", family="Roboto Condensed")
#dev.off()
```

We can see that the *Clean* river topology has fewer nodes, but overall it is more accurate easier to assess. 

## Snap Points to Nearest Line

Now that we have a river network, we can snap points to the nearest line, and use that to calculate distances between sites along the river network.

```{r snap_xyData, eval=T, echo=F}

# get h8
h8 <- read_sf(unzip("data/shps/h8_AMR_BEA_YUB.zip"), quiet = T) %>%
  st_transform(crs=4326) #%>% 
  #filter(HU_8_NAME=="North Fork American")
file.remove(list.files(pattern = "h8_AMR_BEA_YUB*",recursive = F))

# select HUC8
h8_nfa <- h8 %>% filter(HU_8_NAME=="North Fork American")

# get map range from this layer for flowlines call
mapRange <- c(range(st_coordinates(h8_nfa)[,1]),range(st_coordinates(h8_nfa)[,2]))

# get XY info in:
metadat<- read_csv("data_output/rapture06_metadata_revised.csv") %>% 
  filter(!is.na(lon), SPP_ID=="RABO" | SPP_pc1=="RABO") %>% # get rid of NAs and non-RABO
  #filter(grepl("NFA|MFA|RUB|NFMFA", Locality)) %>% 
  filter(grepl("NFY|MFY|SFY", Locality)) %>%
  #filter(grepl("BEAR", Locality)) %>% 
  select(-HU_8_NAME, -HUC_8) %>%  # drop conflicting cols
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%  # make sf
  st_transform(crs = us_aeqd_proj) # convert to utms:

# add COORDS
metadat$X <- st_coordinates(metadat)[,1]
metadat$Y <- st_coordinates(metadat)[,2]

# run this to snap points to line (and see distrib)
frog_riv <- xy2segvert(x=metadat$X, y=metadat$Y, rivers=rivs_fixed)
head(frog_riv)
hist(frog_riv$snapdist/1000, breaks=50,main="Point Snapping dist. (km)", col="skyblue3", xlab="Snapping Distance", family="Roboto Condensed")

# add vertices
metadat <- bind_cols(metadat, frog_riv) %>% 
  distinct(Locality,.keep_all = T)

# POINTS ON MAP
plot(x=rivs_fixed)
#zoomtoseg(seg=c(63, 7), rivers=rivs_fixed)
points(metadat$X, metadat$Y, pch=21, col="red") # raw coords
riverpoints(seg=frog_riv$seg, vert=frog_riv$vert, rivers=rivs_fixed, pch=22, cex=0.5,
            bg="skyblue") # snapped coords
text(metadat$X, metadat$Y, labels=metadat$vert, pos = 4, family="Roboto Condensed")
#text(metadat$X, metadat$Y, labels=metadat$Locality, col="maroon", pos = 2, family="Roboto Condensed")

# DETECT ROUTES
#detectroute(start=465, end=5, rivers=rivs_fixed)
# riverdistance(startseg=207, startvert=6, endseg=129, endvert=24,
#               rivers=rivs_fixed, map=TRUE)
#riverdistance(startseg=31, startvert=9, endseg=185, endvert=5,
#              rivers=rivs_fixed, map=TRUE)
```

#### Create Distance Matrix

Once points are snapped to the line network, it is possible to extract distances between site pairs, and export.

```{r create_dist_matrix, eval=T, echo=T}

# CREATE MATRIX OF DISTS
dmat <- riverdistancemat(metadat$seg, metadat$vert, rivs_fixed, ID = metadat$Locality ) %>% as.matrix
head(dmat)

# dist matrix in heatmap form
image(1:nrow(dmat), 1:ncol(dmat), dmat, axes = FALSE, 
      xlab="", ylab="", col = terrain.colors(100))
axis(1, 1:nrow(dmat), rownames(dmat), cex.axis = 0.7, las=3, family="Roboto Condensed")
axis(2, 1:nrow(dmat), colnames(dmat), cex.axis = 0.5, las=1, family="Roboto Condensed")
text(expand.grid(1:nrow(dmat), 1:ncol(dmat)), sprintf("%0.1f", dmat/1000), cex=0.6, family="Roboto Condensed")
graphics::title("Mainstem River Distance (km) for NF American Watershed Sites", cex.main=1, family="Roboto Condensed")

nfa_dists <- as.data.frame(dmat)
write_tsv(nfa_dists, "data_output/mainstem_distance_matrix_NFA.txt")
```

#### Merge with Existing Data

Now merge this with the existing data set. In this case we are adding mainstem distances (in addition to total distances).

First calculate the mean distance across all pairwise combinations for each site. 

```{r mean_dists, echo=T, eval=F}

load("data_output/fst_dists_all.rda") # full fst_means

#load("data_output/mod_theta100k_fst_dists_yub_amer_bear.rda") # mean values for all

# AMER dist_matrix -------------------------

# read in dist matrix: NFA American HUC8 (NFA, and MFA)
dist_amer <-read_tsv("data_output/mainstem_distance_matrix_AMER.txt") %>% 
  as.matrix()
colnames(dist_amer) <- gsub("-", replacement = "_", colnames(dist_amer))
colnames(dist_amer)<-tolower(colnames(dist_amer))
rownames(dist_amer)<-colnames(dist_amer)

# NFA: calc rowmeans for single site 
dist_nfa <- dist_amer[grepl("nfa",rownames(dist_amer)),grepl("nfa",colnames(dist_amer))]
dist_nfa_means <- tibble("sites"=rownames(dist_nfa),"mean_main_km"= rowMeans(dist_nfa)/1000)

dist_nfa_means$sites <- gsub("nfa_euch_ds", "nfa_euch", dist_nfa_means$sites)
print(dist_nfa_means)


# combine all
dist_all_means <- bind_rows(dist_nfy_means, dist_sfy_means,
                            dist_bear_means, dist_nfa_means,
                            dist_mfa_means)
#write_csv(dist_all_means, path="data_output/mainstem_dists_all.csv")

```

Then take that info and combine with the orig `fst_dist` dataset.

```{r merge_dists, echo=T, eval=F}

# join with 
load("data_output/fst_dists_all.rda") # full fst_means for all sites

(fst_dists_comb <- left_join(fst_dists, dist_all_means, by="sites"))

# add the missing values for MFY OH to Oregon (9.67)
fst_dists_comb[12,6] <- 9.67
# add the missing values for NFA_EUCH (mean of us and ds dist: (20.353)
fst_dists_comb[15,6] <- 20.353

# add the missing values for SFA (arbitary 25 for now since single comparison across watershed)
fst_dists_comb[11,6] <- 25

# reorder: 
fst_dists_comb <- fst_dists_comb %>% 
  select(sites:mean_km, mean_main_km,REG, watershed)

#save(fst_dists_comb, file = "data_output/fst_dist_all_mainstem.rda")

```

Can also merge with other metadat/mod data sets.

```{r merge_mod_meandist, eval=F, echo=T}

load("data_output/fst_dist_all_mainstem.rda")
load("data_output/thetas100k_by_site.rda")

#load("data_output/mod_theta100k_fst_dists_yub_amer_bear.rda")

# add the additional metadata:
load("data_output/frog_pts_nhd_comids.rda")

# now fix fst/theta table
fst_dists_comb$Locality <- toupper(fst_dists_comb$sites)

# need to make localities match:
thetas100k$Locality <- toupper(thetas100k$ID)

# check site names:
unique(thetas100k$Locality) %>% sort
unique(fst_dists_comb$Locality) %>% sort

# join
theta100k_fst_dists <- right_join(thetas100k, fst_dists_comb, by="Locality") %>% dplyr::select(Locality, river, watershed, Tw:nsites, REG, fst_mean:mean_main_km, watershed, -reg, -sites, -Tj, -ID, -Tdiff)

# rename NFA_EUCH so it will join with spatial data
theta100k_fst_dists$Locality <- gsub("NFA_EUCH", "NFA_EUCHDS", theta100k_fst_dists$Locality)

# add SCOTCHMAN ck thetas:
theta100k_fst_dists[28,c(4:6)] <- c(0.00666052, 0.00679309, 1947711)

# fix cols
theta100k_fst_dists$Tdiff <- (theta100k_fst_dists$Tp - theta100k_fst_dists$Tw)

theta100k_fst_dists <- select(theta100k_fst_dists, Locality:watershed, Tdiff, Tw:mean_main_km)

# write fst & thetas only
#save(theta100k_fst_dists, file="data_output/theta100k_fst_dists_yub_amer_bear.rda")

# add the additional metadata:
load("data_output/frog_pts_nhd_comids.rda")

# select and fill NA's in ecoregion with Sierra Nevada
frog_pts_nhd <- frog_pts_nhd %>%
  dplyr::select(Locality:SPP_ID,lat:NHD_Tot_DA_sqkm) %>% 
  mutate(EcoRegion=if_else(is.na(EcoRegion), "Sierra Nevada", EcoRegion),
         Locality=gsub("-", "_", Locality))

# rename a few sites:
frog_pts_nhd$Locality <- gsub("BEAR_STHC", "BEAR_STHO", frog_pts_nhd$Locality)
frog_pts_nhd$Locality <- gsub("BEAR_STHA", "BEAR_STHO_HAW", frog_pts_nhd$Locality)
frog_pts_nhd$Locality <- gsub("RUB_LC_US", "RUB_LCUS", frog_pts_nhd$Locality)
frog_pts_nhd$Locality <- gsub("SFY_RockCk","SFY_ROCKCK", frog_pts_nhd$Locality)
frog_pts_nhd$Locality <- gsub("SFY_Scotchman","SFY_SCOTCHMAN", frog_pts_nhd$Locality)
frog_pts_nhd$Locality <- gsub("SFY_ShadyCk","SFY_SHADYCK", frog_pts_nhd$Locality)
frog_pts_nhd$Locality <- gsub("SFY_SpringCk","SFY_SPRINGCK", frog_pts_nhd$Locality)

# join with genomic data: (try left_join here but will have NA's)
mod_dat_nhd <- inner_join(theta100k_fst_dists, frog_pts_nhd, by="Locality") %>% distinct(Locality, .keep_all = T)

# write fst & thetas only
#save(mod_dat_nhd, file="data_output/mod_theta100k_fst_dists_yub_amer_bear.rda")

```


Finally, we can map these data with a terrain or google map background.

```{r simple_ggmap, echo=F, eval=T}



# SIMPLE MAP --------------------------------------------------------------

library(ggmap) # doesn't plot with SF so need to convert
location<-c(mean(st_coordinates(h8_nfa)[,1]),
            mean(st_coordinates(h8_nfa)[,2]))
map1 <- get_map(location=location,crop = F,
                color="bw",
                maptype="terrain",
                source="google",
                zoom=9)

terrain_bg <-ggmap(map1)

# convert metadat to WGS again and add coords
metadat <- metadat %>% st_transform(crs = 4326)
metadat$long <- st_coordinates(metadat)[,1]
metadat$lat <- st_coordinates(metadat)[,2]

# convert river network:
rivs_alb <- st_read("data/shps/rivs_nhd4_nfa.shp", quiet = T) %>% st_transform(crs=4326) %>% 
  as("Spatial") %>% fortify

# convert h8_nfa
h8_nfa.sp <- h8_nfa %>% as("Spatial") %>% fortify

terrain_bg + 
  scale_color_viridis_d() +
  geom_path(data=rivs_alb, aes(x=long, y=lat, group=group), col="blue")+
  labs(x="Longitude (WGS84)", y="Latitude") +   theme_bw() +
  geom_polygon(data=h8_nfa.sp, aes(x=long, y=lat, group=group), fill=NA, color="maroon", lwd=1.2, alpha=0.5) +
  geom_point(data=metadat, aes(x=long, y=lat), fill="yellow", pch=21, size=3) +
  coord_fixed(xlim = mapRange[c(1:2)], ylim = mapRange[c(3:4)], ratio=1.3)

```

